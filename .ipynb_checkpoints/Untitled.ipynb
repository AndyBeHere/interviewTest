{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path1 = '.\\data\\Test1_features.dat'\n",
    "data_f = pd.read_table(path1,header=None,sep=',')\n",
    "\n",
    "# data = pd.DataFrame(data,dtype=np.float)\n",
    "data_f = np.array(data_f)\n",
    "\n",
    "data_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '.\\data\\Test1_labels.dat'\n",
    "data_l = pd.read_table(path2,header=None,sep=',')\n",
    "\n",
    "# data = pd.DataFrame(data,dtype=np.float)\n",
    "data_l = np.array(data_l)\n",
    "\n",
    "np.sum(data_l==1)\n",
    "np.sum(data_l==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "# load data as array\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data('./data/Test1_features.dat')\n",
    "dataset.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "随机森林算法，组合算法bagging(装袋)的一种\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "from cart_clf import CART_CLF\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_tree=6, n_fea=None, ri_rc=True, L=None, epsilon=1e-3, min_sample=1):\n",
    "        self.n_tree = n_tree\n",
    "        self.n_fea = n_fea  # 每棵树中特征的数量\n",
    "        self.ri_rc = ri_rc  # 判定特征的选择选用RI还是RC, 特征比较少时使用RC\n",
    "        self.L = L # 选择RC时，进行线性组合的特征个数\n",
    "        self.tree_list = []  # 随机森林中子树的list\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.min_sample = min_sample  # 叶节点含有的最少样本数\n",
    "\n",
    "        self.D = None  # 输入数据维度\n",
    "        self.N = None\n",
    "\n",
    "    def init_param(self, X_data):\n",
    "        # 初始化参数\n",
    "        self.D = X_data.shape[1] # features维数\n",
    "        self.N = X_data.shape[0] # dataset样本数\n",
    "        if self.n_fea is None:\n",
    "            self.n_fea = int(math.log2(self.D) + 1)  # 默认选择特征的个数\n",
    "        return\n",
    "\n",
    "    def extract_fea(self):\n",
    "        # 从原数据中抽取特征(RI)或线性组合构建新特征(RC)\n",
    "        if self.ri_rc:\n",
    "            if self.n_fea > self.D:\n",
    "                raise ValueError('the number of features should be lower than dimention of data while RI is chosen')\n",
    "            fea_arr = np.random.choice(self.D, self.n_fea, replace=False)\n",
    "        else:\n",
    "            fea_arr = np.zeros((self.n_fea, self.D))\n",
    "            for i in range(self.n_fea):\n",
    "                out_fea = np.random.choice(self.D, self.L, replace=False)\n",
    "                coeff = np.random.uniform(-1, 1, self.D)  # [-1,1]上的均匀分布来产生每个特征前的系数\n",
    "                coeff[out_fea] = 0\n",
    "                fea_arr[i] = coeff\n",
    "        return fea_arr\n",
    "\n",
    "    def extract_data(self, X_data, y_data):\n",
    "        # 从原数据中有放回的抽取样本，构成每个决策树的自助样本集\n",
    "        fea_arr = self.extract_fea()  # col_index or coeffs\n",
    "        inds = np.unique(np.random.choice(self.N, self.N))  # row_index, 有放回抽取样本\n",
    "        sub_X = X_data[inds]\n",
    "        sub_y = y_data[inds]\n",
    "        if self.ri_rc:\n",
    "            sub_X = sub_X[:, fea_arr]\n",
    "        else:\n",
    "            sub_X = sub_X @ fea_arr.T\n",
    "        return sub_X, sub_y, fea_arr\n",
    "\n",
    "    def fit(self, X_data, y_data):\n",
    "        # 训练主函数\n",
    "        self.init_param(X_data)\n",
    "        for i in range(self.n_tree):\n",
    "            sub_X, sub_y, fea_arr = self.extract_data(X_data, y_data)\n",
    "            subtree = CART_CLF(epsilon=self.epsilon, min_sample=self.min_sample)\n",
    "            subtree.fit(sub_X, sub_y)\n",
    "            self.tree_list.append((subtree, fea_arr))  # 保存训练后的树及其选用的特征，以便后续预测时使用\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 预测，多数表决\n",
    "        res = defaultdict(int)  # 存储每个类得到的票数\n",
    "        for item in self.tree_list:\n",
    "            subtree, fea_arr = item\n",
    "            if self.ri_rc:\n",
    "                X_modify = X[fea_arr]\n",
    "            else:\n",
    "                X_modify = (np.array([X]) @ fea_arr.T)[0]\n",
    "            label = subtree.predict(X_modify)\n",
    "            res[label] += 1\n",
    "        return max(res, key=res.get)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    data = load_iris()\n",
    "    X_data = data['data']\n",
    "    y_data = data['target']\n",
    "    from machine_learning_algorithm.cross_validation import validate\n",
    "\n",
    "    g = validate(X_data, y_data, ratio=0.2)\n",
    "    for item in g:\n",
    "        X_train, y_train, X_test, y_test = item\n",
    "        RF = RandomForest(n_tree=50, n_fea=2, ri_rc=True)\n",
    "        RF.fit(X_train, y_train)\n",
    "        score = 0\n",
    "        for X, y in zip(X_test, y_test):\n",
    "            if RF.predict(X) == y:\n",
    "                score += 1\n",
    "        print(score / len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from csv import reader\n",
    "\n",
    "#path='K:\\\\pycode\\\\regression\\\\POR_new\\\\POR_DX14\\\\DX14_ALL.csv'\n",
    "#data_r = pd.read_csv(path,delimiter=',')\n",
    "#X1=data_r[['GR','SP','CAL','RT','RI','RXO','AC','DEN','CNL']]\n",
    "#y1= data_r['POR']\n",
    "#x= np.array(X1,dtype='float64')\n",
    "#y= np.array(y1,dtype='float64')\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.7,random_state=1)\n",
    "#print(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# load data as array\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "def run_kfold(clf, X, y, n_folds):\n",
    "    kf = KFold(n_folds)\n",
    "    outcomes = []\n",
    "    fold = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold = fold + 1\n",
    "        X_train,X_test = X[train_index],X[test_index]\n",
    "        y_train,y_test = y[train_index],y[test_index]\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "        pre = clf.predict(X_test)\n",
    "        prob_pre = clf.predict_proba(X_test) #给出带有概率值的结果，每个点所有label的概率和为1\n",
    "        pre_val = prob_pre[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, pre_val)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        outcomes.append(roc_auc)\n",
    "\n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    \n",
    "    return mean_outcome\n",
    "\n",
    "\n",
    "\n",
    "def test_d_m():\n",
    "    %matplotlib inline\n",
    "    path_f = './data/Test1_features.dat'\n",
    "    path_l = './data/Test1_labels.dat'\n",
    "#     data = np.loadtxt(path, delimiter=',', dtype=float, skiprows=1, converters={0:lith_type})\n",
    "    X, y = (load_data(path_f), load_data(path_l).ravel())\n",
    "#     X_train,  X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=0)\n",
    "#     maxdepths = range(1,9)\n",
    "    min_samples_leaf = range(40,60,1)\n",
    "    max_features=np.linspace(0.1,1)\n",
    "    scores = []\n",
    "    i=0\n",
    "    for min_samples in min_samples_leaf:\n",
    "        for max_feature in max_features:\n",
    "            regr = ensemble.RandomForestClassifier(max_depth=3,max_features=max_feature,min_samples_leaf=min_samples)\n",
    "#             regr.fit(X_train, y_train)\n",
    "            \n",
    "            scores.append(run_kfold(regr,X,y, 3))\n",
    "            i=i+1\n",
    "            print('step:',i)\n",
    "            print(scores)\n",
    "    ##绘图\n",
    "    min_samples_leaf,max_features = np.meshgrid(min_samples_leaf,max_features)\n",
    "    scores = np.array(scores).reshape(min_samples_leaf.shape)\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib import cm\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    surf = ax.plot_surface(min_samples_leaf,max_features,scores,rstride=1,cstride=1,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    fig.colorbar(surf,shrink=0.5,aspect=5)\n",
    "    ax.set_xlabel(r\"$\\alpha$\")\n",
    "    ax.set_ylabel(r\"$\\rho$\")\n",
    "    ax.set_zlabel(\"score\")\n",
    "    ax.set_title(\"题目一：随机森林3-fold的AUC评分\")\n",
    "    plt.show()\n",
    "    \n",
    "#x_train,x_test,y_train,y_test = data()\n",
    "test_d_m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(2,8,2)\n",
    "b = np.linspace(1,1,2)\n",
    "a,b = np.meshgrid(a,b)\n",
    "c = np.array([1,2,3,4,5,6]).reshape(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import  MultipleLocator\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "# Grab some test data.\n",
    "X, Y, Z = axes3d.get_test_data(0.05)\n",
    "xmajorLocator = MultipleLocator(1)\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "ax.xaxis.grid(True, which='major')\n",
    " \n",
    "# Plot a basic wireframe.\n",
    "ax.plot_wireframe(X, Y, Z, rstride=1, cstride=10)\n",
    "\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInitSet(dataSet):\n",
    "    retDict = {}\n",
    "    for trans in dataSet:\n",
    "        retDict[frozenset(trans)] = 1\n",
    "    return retDict\n",
    "createInitSet(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# FP树构建函数\n",
    "# 使用数据集以及最小支持度作为参数来构建FP树。树构建过程会遍历数据集两次。\n",
    "def createTree(dataSet, minSup=1) :\n",
    "    headerTable = {}\n",
    "    # 第一次遍历扫描数据集并统计每个元素项出现的频度。这些信息被保存在头指针中。\n",
    "    for trans in dataSet :\n",
    "        for item in trans :\n",
    "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
    "    # 接着扫描头指针表删除那些出现次数小于minSup的项。\n",
    "    for k in headerTable.keys() :\n",
    "        if headerTable[k] < minSup :\n",
    "            del(headerTable[k])\n",
    "    freqItemSet = set(headerTable.keys())\n",
    "    # 如果所有项都不频繁，无需下一步处理\n",
    "    if len(freqItemSet) == 0 : return None, None\n",
    "    # 对头指针表稍加扩展以便可以保存计数值及指向每种类型第一个元素项的指针\n",
    "    for k in headerTable :\n",
    "        headerTable[k] = [headerTable[k], None]\n",
    "    # 创建只包含空集合的根节点\n",
    "    retTree = treeNode('Null Set', 1, None)\n",
    "    for tranSet, count in dataSet.items() :\n",
    "        localD = {}\n",
    "        # 根据全局频率对每个事务中的元素进行排序\n",
    "        for item in tranSet :\n",
    "            if item in freqItemSet :\n",
    "                localD[item] = headerTable[item][0]\n",
    "        if len(localD) > 0 :\n",
    "            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p : p[1], reverse=True)]\n",
    "            # 排序后，调用updateTree()方法\n",
    "            updateTree(orderedItems, retTree, headerTable, count)\n",
    "    return retTree, headerTable\n",
    "\n",
    "# 为了让FP树生长，需调用updateTree函数。\n",
    "def updateTree(items, inTree, headerTable, count) :\n",
    "    # 该函数首先测试事务中的第一个元素项是否作为子节点存在。\n",
    "    if items[0] in inTree.children :\n",
    "        # 如果存在，则更新该元素项的计数\n",
    "        inTree.children[items[0]].inc(count)\n",
    "    else :\n",
    "        # 如果不存在，则创建一个新的treeNode并将其作为一个子节点添加到树中，这时，头指针表也要更新以指向新的节点。\n",
    "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
    "        if headerTable[items[0]][1] == None :\n",
    "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
    "        else :\n",
    "            # 更新头指针表需要调用函数updateHeader\n",
    "            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "    # updateTree()完成的最后一件事是不断迭代调用自身，每次调用时会去掉列表中的第一个元素\n",
    "    if len(items) > 1 :\n",
    "        updateTree(items[1::], inTree.children[items[0]], headerTable, count)\n",
    "\n",
    "# 确保节点链接指向树中该元素项的每一个实例，从头指针的nodeLink开始，一直沿着nodeLink直到到达链表末尾。\n",
    "# 当处理树的时候，一种自然的反应就是迭代完整每一件事。当以相同方式处理链表时可能会遇到一些问题，\n",
    "# 原因是如果链表很长可能会遇到迭代调用的次数限制\n",
    "def updateHeader(nodeToTest, targetNode) :\n",
    "    while (nodeToTest.nodeLink != None) :\n",
    "        nodeToTest = nodeToTest.nodeLink\n",
    "    nodeToTest.nodeLink = targetNode\n",
    "--------------------- \n",
    "作者：namelessml \n",
    "来源：CSDN \n",
    "原文：https://blog.csdn.net/namelessml/article/details/52871778 \n",
    "版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from csv import reader\n",
    "\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "data = load_data('./data/Test2_Data.csv')\n",
    "height = data[0]\n",
    "data = data[1:,:]\n",
    "n_row, n_col = data.shape\n",
    "print(n_row,n_col)\n",
    "# dataset = np.zeros([n_row,n_col])\n",
    "# dataset = dataset.tolist()\n",
    "# data.tolist()\n",
    "# dataset.dtype = 'string'\n",
    "a = 0\n",
    "dataSet = []\n",
    "for i in range(n_row):\n",
    "    dataSet.append([])\n",
    "    for j in range(n_col):\n",
    "        if data[i][j] != 0 and data[i][j] != '0' and data[i][j] != 'FALSE':\n",
    "            dataSet[a].append(height[j])\n",
    "    a = a + 1\n",
    "            \n",
    "dataSet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInitSet(dataSet):\n",
    "    retDict = {}\n",
    "    for trans in dataSet:\n",
    "        retDict[frozenset(trans)] = 1\n",
    "    return retDict\n",
    "\n",
    "class treeNode:\n",
    "    def __init__(self, nameValue, numOccur, parentNode):\n",
    "        self.name = nameValue\n",
    "        self.count = numOccur\n",
    "        self.nodeLink = None\n",
    "        self.parent = parentNode\n",
    "        self.children = {}\n",
    "\n",
    "    def inc(self, numOccur):\n",
    "        self.count += numOccur\n",
    "\n",
    "    def disp(self, ind=1):\n",
    "        print('  '*ind, self.name, ' ', self.count)\n",
    "        for child in self.children.values():\n",
    "            child.disp(ind+1)\n",
    "\n",
    "def updateHeader(nodeToTest, targetNode):\n",
    "    while nodeToTest.nodeLink != None:\n",
    "        nodeToTest = nodeToTest.nodeLink\n",
    "    nodeToTest.nodeLink = targetNode\n",
    "def updateFPtree(items, inTree, headerTable, count):\n",
    "    if items[0] in inTree.children:\n",
    "        # 判断items的第一个结点是否已作为子结点\n",
    "        inTree.children[items[0]].inc(count)\n",
    "    else:\n",
    "        # 创建新的分支\n",
    "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
    "        # 更新相应频繁项集的链表，往后添加\n",
    "        if headerTable[items[0]][1] == None:\n",
    "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
    "        else:\n",
    "            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "    # 递归\n",
    "    if len(items) > 1:\n",
    "        updateFPtree(items[1::], inTree.children[items[0]], headerTable, count)\n",
    "\n",
    "def createFPtree(dataSet, minSup=1):\n",
    "    headerTable = {}\n",
    "    for trans in dataSet:\n",
    "        for item in trans:\n",
    "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
    "    for k,v in headerTable.items() :\n",
    "        if k == 'Labels' :\n",
    "            headerTable = {k:v}\n",
    "    freqItemSet = set(headerTable.keys()) # 满足最小支持度的频繁项集\n",
    "    if len(freqItemSet) == 0:\n",
    "        return None, None\n",
    "    for k in headerTable:\n",
    "        headerTable[k] = [headerTable[k], None] # element: [count, node]\n",
    "\n",
    "    retTree = treeNode('Null Set', 1, None)\n",
    "    for tranSet, count in dataSet.items():\n",
    "        # dataSet：[element, count]\n",
    "        localD = {}\n",
    "        for item in tranSet:\n",
    "            if item in freqItemSet: # 过滤，只取该样本中满足最小支持度的频繁项\n",
    "                localD[item] = headerTable[item][0] # element : count\n",
    "        if len(localD) > 0:\n",
    "            # 根据全局频数从大到小对单样本排序\n",
    "            orderedItem = [v[0] for v in sorted(localD.items(), key=lambda p:p[1], reverse=True)]\n",
    "            # 用过滤且排序后的样本更新树\n",
    "            updateFPtree(orderedItem, retTree, headerTable, count)\n",
    "    return retTree, headerTable\n",
    "\n",
    "def ascendTree(leafNode, prefixPath) :\n",
    "    # 迭代上溯整棵树\n",
    "    if leafNode.parent != None :\n",
    "        prefixPath.append(leafNode)\n",
    "        ascendTree(leafNode.parent, prefixPath)\n",
    "\n",
    "# 遍历链表直到到达结尾。每遇到一个元素项都会调用ascendTree()来上溯FP树，并收集所有遇到的元素项的名称。\n",
    "# 该列表返回之后添加到条件模式基字典condPats中\n",
    "def findPrefixPath(basePat, treeNode) :\n",
    "    condPats = {}\n",
    "    while treeNode != None :\n",
    "        prefixPath = []\n",
    "        ascendTree(treeNode, prefixPath)\n",
    "        if len(prefixPath) > 1 :\n",
    "            condPats[frozenset(prefixPath[1:])] = treeNode.count\n",
    "        treeNode = treeNode.nodeLink\n",
    "    return condPats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minSup = len(dataSet)*0.1\n",
    "initSet = createInitSet(dataSet)\n",
    "myFPtree, myHeaderTab = createFPtree(initSet, minSup) # 最小支持度3\n",
    "myFPtree.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mineFPtree(inTree, headerTable, minSup, preFix, freqItemList):\n",
    "    # 最开始的频繁项集是headerTable中的各元素\n",
    "    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p:str(p[1]))] # 根据频繁项的总频次排序\n",
    "    for basePat in bigL: # 对每个频繁项\n",
    "        newFreqSet = preFix.copy()\n",
    "        newFreqSet.add(basePat)\n",
    "        freqItemList.append(newFreqSet)\n",
    "        condPattBases = findPrefixPath(basePat, headerTable[basePat][1]) # 当前频繁项集的条件模式基\n",
    "        myCondTree, myHead = createFPtree(condPattBases, minSup) # 构造当前频繁项的条件FP树\n",
    "        if myHead != None:\n",
    "            # print 'conditional tree for: ', newFreqSet\n",
    "            # myCondTree.disp(1)\n",
    "            mineFPtree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqItems = []\n",
    "mineFPtree(myFPtree, myHeaderTab, 120, set([]), freqItems)\n",
    "for x in freqItems:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [1,3,5,6,7]\n",
    "b= np.random.randint(0,5)\n",
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymining import itemmining, assocrules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def dataSet(data):\n",
    "    height = data[0]\n",
    "    data = data[1:,:]\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0 and data[i][j] != '0' and data[i][j] != 'FALSE':\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "\n",
    "data = load_data('./data/Test2_Data.csv')\n",
    "dataSet = dataSet(data)\n",
    "minSup = len(dataSet)*0.1\n",
    "\n",
    "transactions = dataSet\n",
    "relim_input = itemmining.get_relim_input(transactions)\n",
    "report = itemmining.relim(relim_input, min_support=minSup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules1 = assocrules.mine_assoc_rules(report, min_support=120, min_confidence=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import orangecontrib.associate.fpgrowth as oaf  #进行关联规则分析的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def dataSet(data):\n",
    "    height = data[0]\n",
    "    data = data[1:,:]\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0 and data[i][j] != '0' and data[i][j] != 'FALSE':\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "\n",
    "data = load_data('./data/Test2_Data.csv')\n",
    "dataSet = dataSet(data)\n",
    "minSup = len(dataSet)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " supportRate = 0.1\n",
    "confidenceRate = 0.7    \n",
    "itemsets = dict(oaf.frequent_itemsets(dataSet, supportRate))\n",
    "rules = oaf.association_rules(itemsets, confidenceRate)\n",
    "rules = list(rules)\n",
    "regularNum = len(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import orangecontrib.associate.fpgrowth as oaf  #进行关联规则分析的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filename):\n",
    "    data = pd.read_table(filename,header=None,sep=',')\n",
    "    dataset = np.array(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def dataSet(data):\n",
    "    height = data[0]\n",
    "    data = data[1:,:]\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0 and data[i][j] != '0' and data[i][j] != 'FALSE':\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "\n",
    "data = load_data('./data/Test2_Data.csv')\n",
    "data.\n",
    "\n",
    "# dataSet = np.zeros(data.shape)\n",
    "# for index,item in data[1:,:]:\n",
    "#     for index2,items in item:\n",
    "#         if items != 'FALSE' and items != 'TRUE':\n",
    "#             int(items)\n",
    "#         elif items == 'FALSE':\n",
    "#             items == int(0)\n",
    "#         else:\n",
    "#             items == int(1)\n",
    "            \n",
    "            \n",
    "data.columns\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataSet(data,head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import orangecontrib.associate.fpgrowth as oaf  #进行关联规则分析的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')\n",
    "# head = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')[0]\n",
    "head = np.array(data)[0]\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=0,sep=',')\n",
    "\n",
    "for u in data.columns:\n",
    "        data[u] = data[u].astype('int')\n",
    "    \n",
    "# row_add = np.array(data).sum(axis=0)\n",
    "\n",
    "dataSet = np.array(data)\n",
    "new_data = []\n",
    "row_remain = []\n",
    "head_remain = []\n",
    "\n",
    "n_column = np.array(data).shape[1]\n",
    "# row_reserve = []\n",
    "for j in range(n_column):\n",
    "    a = np.zeros((n_column))\n",
    "    for i in range(len(data)):\n",
    "        if dataSet[i][j] != 0:\n",
    "#             print(dataSet[i])\n",
    "#             print(a)\n",
    "            a = a + dataSet[i]\n",
    "    \n",
    "    if a[0] >= 120 and a[-1] >= 120 and max(a[1:-1]) >= 120:\n",
    "        print(j)\n",
    "        new_data.append(dataSet[:,j])\n",
    "        row_remain.append(j)\n",
    "\n",
    "        head_remain.append(head[j])\n",
    "        \n",
    "data = np.array(new_data).transpose()\n",
    "head_new = data[0,:]      \n",
    "\n",
    "head = np.array(head_remain)\n",
    "\n",
    "def dataSet(data,head):\n",
    "    height = head\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    print(head.shape)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0:\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "# print(np.array(data))\n",
    "# print(np.array(head))\n",
    "dataSet = dataSet(data, head)\n",
    "\n",
    "supportRate = 0.3\n",
    "confidenceRate = 0.7\n",
    "\n",
    "itemsets = dict(oaf.frequent_itemsets(dataSet, supportRate))\n",
    "rules = [(P, Q, supp, conf) for P, Q, supp, conf in oaf.association_rules(itemsets, 0.7) if len(P) >= 2 and len(Q)==1 ]\n",
    "rules = list(rules)\n",
    "regularNum = len(rules)\n",
    "rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = [[1,2,3],[4,5,6]]\n",
    "numpy.array(a).transpose()\n",
    "a[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3],[2,3,4]]) \n",
    "a[0] + np.array([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import orangecontrib.associate.fpgrowth as oaf  #进行关联规则分析的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyfpgrowth as fp\n",
    "\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')\n",
    "# head = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')[0]\n",
    "head = np.array(data)[0]\n",
    "\n",
    "for i in range(len(head)):\n",
    "    head[i] = i\n",
    "\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=0,sep=',')\n",
    "\n",
    "for u in data.columns:\n",
    "    data[u] = data[u].astype('int')\n",
    "    \n",
    "# row_add = np.array(data).sum(axis=0)\n",
    "\n",
    "dataSet = np.array(data)\n",
    "new_data = []\n",
    "row_remain = []\n",
    "head_remain = []\n",
    "\n",
    "n_column = np.array(data).shape[1]\n",
    "# row_reserve = []\n",
    "for j in range(n_column):\n",
    "    a = np.zeros((n_column))\n",
    "    for i in range(len(data)):\n",
    "        if dataSet[i][j] != 0:\n",
    "#             print(dataSet[i])\n",
    "#             print(a)\n",
    "            a = a + dataSet[i]\n",
    "    \n",
    "    if a[0] >= 120 and a[-1] >= 120 and max(a[1:-1]) >= 120:\n",
    "        print(j)\n",
    "        new_data.append(dataSet[:,j])\n",
    "        row_remain.append(j)\n",
    "\n",
    "        head_remain.append(head[j])\n",
    "        \n",
    "data = np.array(new_data).transpose()\n",
    "head_new = data[0,:]      \n",
    "\n",
    "head = np.array(head_remain)\n",
    "\n",
    "def dataSet(data,head):\n",
    "    height = head\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    print(head.shape)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0:\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "# print(np.array(data))\n",
    "# print(np.array(head))\n",
    "dataSet = dataSet(data, head)\n",
    "\n",
    "supportRate = 0.1\n",
    "confidenceRate = 0.7\n",
    "\n",
    "patterns = fp.find_frequent_patterns(dataSet, 120)\n",
    "rules = fp.generate_association_rules(patterns, 0.7)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named orangecontrib.associate.fpgrowth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8e08d5582a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0morangecontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massociate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpgrowth\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moaf\u001b[0m  \u001b[1;31m#进行关联规则分析的包\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyfpgrowth\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named orangecontrib.associate.fpgrowth"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import orangecontrib.associate.fpgrowth as oaf  #进行关联规则分析的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyfpgrowth as fp\n",
    "import apriori\n",
    "\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')\n",
    "# head = pd.read_table('./data/Test2_Data.csv',header=None,sep=',')[0]\n",
    "head = np.array(data)[0]\n",
    "data = pd.read_table('./data/Test2_Data.csv',header=0,sep=',')\n",
    "\n",
    "for u in data.columns:\n",
    "        data[u] = data[u].astype('int')\n",
    "    \n",
    "# row_add = np.array(data).sum(axis=0)\n",
    "\n",
    "dataSet = np.array(data)\n",
    "new_data = []\n",
    "row_remain = []\n",
    "head_remain = []\n",
    "\n",
    "n_column = np.array(data).shape[1]\n",
    "# row_reserve = []\n",
    "for j in range(n_column):\n",
    "    a = np.zeros((n_column))\n",
    "    for i in range(len(data)):\n",
    "        if dataSet[i][j] != 0:\n",
    "#             print(dataSet[i])\n",
    "#             print(a)\n",
    "            a = a + dataSet[i]\n",
    "    \n",
    "    if a[0] >= 120 and a[-1] >= 120 and max(a[1:-1]) >= 120:\n",
    "        print(j)\n",
    "        new_data.append(dataSet[:,j])\n",
    "        row_remain.append(j)\n",
    "\n",
    "        head_remain.append(head[j])\n",
    "        \n",
    "data = np.array(new_data).transpose()\n",
    "head_new = data[0,:]      \n",
    "\n",
    "head = np.array(head_remain)\n",
    "\n",
    "def dataSet(data,head):\n",
    "    height = head\n",
    "    n_row, n_col = data.shape\n",
    "    print(n_row,n_col)\n",
    "    print(head.shape)\n",
    "    # dataset = np.zeros([n_row,n_col])\n",
    "    # dataset = dataset.tolist()\n",
    "    # data.tolist()\n",
    "    # dataset.dtype = 'string'\n",
    "    a = 0\n",
    "    dataSet = []\n",
    "    for i in range(n_row):\n",
    "        dataSet.append([])\n",
    "        for j in range(n_col):\n",
    "            if data[i][j] != 0:\n",
    "                dataSet[a].append(height[j])\n",
    "        a = a + 1\n",
    "    return dataSet\n",
    "# print(np.array(data))\n",
    "# print(np.array(head))\n",
    "dataSet = dataSet(data, head)\n",
    "\n",
    "item_supports, rules = apriori.run_apriori(dataSet, min_confidence=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpDat():\n",
    "    simpDat = [['r', 'z', 'h', 'j', 'p'],\n",
    "               ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
    "               ['z'],\n",
    "               ['r', 'x', 'n', 'o', 's'],\n",
    "               ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n",
    "               ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n",
    "    return simpDat\n",
    "\n",
    "def createInitSet(dataSet):\n",
    "    retDict = {}\n",
    "    for trans in dataSet:\n",
    "        retDict[frozenset(trans)] = 1\n",
    "    return retDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = createInitSet(loadSimpDat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 3,\n",
       " 'j': 1,\n",
       " 'h': 1,\n",
       " 'p': 2,\n",
       " 'z': 5,\n",
       " 'u': 1,\n",
       " 'v': 1,\n",
       " 't': 3,\n",
       " 's': 3,\n",
       " 'w': 1,\n",
       " 'x': 4,\n",
       " 'y': 3,\n",
       " 'o': 1,\n",
       " 'n': 1,\n",
       " 'q': 2,\n",
       " 'e': 1,\n",
       " 'm': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headerTable = {}\n",
    "for trans in dataSet:\n",
    "    for item in trans:\n",
    "        headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
    "headerTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named orangecontrib.associate.fpgrowth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f66736bc5fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0morangecontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massociate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpgrowth\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moaf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named orangecontrib.associate.fpgrowth"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
